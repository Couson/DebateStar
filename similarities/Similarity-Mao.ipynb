{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Similarities\n",
    "\n",
    "This document focuses on single debate and computes similarities between debate text and for/against main points. We aim to find the correlation of similarity scores and win/lose results. \n",
    "\n",
    "Steps:\n",
    "1. Choose a single debate to be analyzed\n",
    "2. Fetch main points and debate speeches from both sides\n",
    "3. Compare both sides's similarity towards the main points\n",
    "\n",
    "This approach can be extended to all debates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fetch Single Debate\n",
    "\n",
    "Debate is selected with its unique debate ID (d+date) and index in the meta data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "meta = pd.read_csv('../Meta Data/metadata_appended_main_points.csv', index_col='id')\n",
    "result = pd.read_csv('../results_data/final_live.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default debate id and index, can be a different one\n",
    "debate_id = 'd20180514'\n",
    "debate = meta.loc[debate_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate ID:  d20180514 \n",
      "Topic:  Automation Will Crash Democracy\n",
      "\n",
      "FOR main points:\n",
      "1 ) the “us versus them” populism sweeping the western world today is fueled by technological advancement: as low- and middle-skilled workers continue to lose jobs to automation, anger will manifest, leaving many concerned that democracy is no longer working in their favor.\n",
      "2 ) the promise of high-paying jobs in the era of automation is a pipe dream. many who lose their jobs won’t have access to the training needed for the sophisticated jobs of the future. this will further widen wealth inequality and exacerbate the divide between globalization’s winners and losers.\n",
      "3 ) anti-democratic leaders promising to bring back jobs from immigrants and robots will continue to get elected over status quo candidates, further eroding democratic institutions and empowering the rise of authoritarian societies.\n",
      "\n",
      "AGAINST main points:\n",
      "1 ) automation won’t mean the end of work, just as the advent of steam power, electricity, and computers didn’t mean the end of work. through regulation, taxation, and innovative solutions like ubi, society can adapt to new technologies and assuage populist discontents.\n",
      "2 ) while automation will displace some workers, this won’t affect the total number of available jobs: as technology continues to advance, newer higher-paying jobs will evolve, employing more people and making the “us versus them” message less appealing.\n",
      "3 ) robots and other technological advances could save the global economy. as the global population ages and birth rates decline, automation can help reshape the future of work, filling demographic-driven job vacancies and staving off a labor shortage that could lead to democratic instability.\n"
     ]
    }
   ],
   "source": [
    "# This prints a nicely formatted single debate info from meta\n",
    "print('Debate ID: ', debate_id, '\\nTopic: ', debate['title'])\n",
    "\n",
    "# Also constructs a list of main points for both sides\n",
    "fmains = []\n",
    "print('\\nFOR main points:')\n",
    "for i, fmain in enumerate(ast.literal_eval(debate['For_Main_Points']), start=1):\n",
    "    fmains.append(fmain.lower())\n",
    "    print(i, ')', fmain.lower())\n",
    "    \n",
    "amains = []\n",
    "print('\\nAGAINST main points:')\n",
    "for i, amain in enumerate(ast.literal_eval(debate['against_Main_Points']), start=1):\n",
    "    amains.append(amain.lower())\n",
    "    print(i, ')', amain.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINNTER:  for\n"
     ]
    }
   ],
   "source": [
    "print('WINNTER: ', result.loc[debate_id]['winner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Speech Text Cleaning on Single Debate\n",
    "\n",
    "Fetch the speeches from both sides of a single debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR side speeches:  32\n",
      "AGAINST side speeches:  31\n"
     ]
    }
   ],
   "source": [
    "# Fetch the debate scripts\n",
    "scripts = pd.read_csv('../For Against Scripts/for_against_scripts_' + debate_id + '.csv')\n",
    "\n",
    "# Construct lists of the speeches by both sides. \n",
    "flist = [s for s in scripts.loc[scripts['side'] == 'for']['script']]\n",
    "print('FOR side speeches: ', len(flist))\n",
    "alist = [s for s in scripts.loc[scripts['side'] == 'against']['script']]\n",
    "print('AGAINST side speeches: ', len(alist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning:** I'd tokenzie all words and sentences, remove puntuations and stopwords. Refer to the cleaning scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# This returns cleaned list of words from given speech string\n",
    "def cleanwords(speech, debug=False): \n",
    "    # tokenize words without punctuations\n",
    "    words = word_tokenize(speech.translate(str.maketrans('', '', string.punctuation)))\n",
    "    # remove meaninglesss words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_words = [w.lower() for w in words if w.lower() not in stop_words]\n",
    "    # remove duplicate words\n",
    "    cleaned_words = list(set(cleaned_words))\n",
    "    if debug:\n",
    "        print('Number of cleaned words:', len(cleaned_words), ', removed words:', len(words) - len(cleaned_words))\n",
    "    return cleaned_words\n",
    "\n",
    "# This returns cleaned list of sentences from given speech string\n",
    "def cleansents(speech, debug=False):  \n",
    "    # tokenized sentences without punctuations\n",
    "    sentences = sent_tokenize(speech)\n",
    "    sentences = [s.translate(str.maketrans('', '', string.punctuation)).lower() for s in sentences]\n",
    "    if debug:\n",
    "        print('Number of cleaned sentences:', len(sentences))\n",
    "    return sentences\n",
    "\n",
    "# This returns list of cleanwords and cleansents from list of speeches\n",
    "def clean(speeches):\n",
    "    cwlst = []\n",
    "    cslst = []\n",
    "    for s in speeches:\n",
    "        cwlst.append(cleanwords(s))\n",
    "        cslst.append(cleansents(s))\n",
    "    return cwlst, cslst\n",
    "\n",
    "# This retuns cleanwords and cleansents from main points\n",
    "def mclean(mains):\n",
    "    speech = \"\"\n",
    "    for m in mains:\n",
    "        speech += m + ' '\n",
    "    cwords = cleanwords(speech)\n",
    "    csents = cleansents(speech)\n",
    "    return cwords, csents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The matrix of cleaned words/sentences of each speech from both sides\n",
    "fcwlst, fcslst = clean(flist)\n",
    "acwlst, acslst = clean(alist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cleaned sentences and words from main points as one single speech\n",
    "mfcwords, mfcsents = mclean(fmains)\n",
    "macwords, macsents = mclean(amains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Similarities\n",
    "\n",
    "This section tries multiple similarity measures to compare speeches with main points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Jaccard Similarity\n",
    "Naive implementation that only checks the intersection of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This computes the intersection; input are two lists of words\n",
    "def jaccard(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "# This averages the jaccard similarities between each speech and the main point\n",
    "#    - cwlst: clean word matrix\n",
    "#    - mcwords: cleaned word list of main point of a side\n",
    "def jaccard_avg(cwlst, mcwords):\n",
    "    s = 0\n",
    "    for cw in cwlst:\n",
    "        jac = jaccard(cw, mcwords)\n",
    "        s += jac\n",
    "    return s / len(cwlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT: for 0.0005937974807516677\n",
      "ACTUAL:  for\n"
     ]
    }
   ],
   "source": [
    "# Most basic prediction\n",
    "fsim = jaccard_avg(fcwlst, mfcwords)\n",
    "asim = jaccard_avg(acwlst, macwords)\n",
    "if  fsim > asim:\n",
    "    print('PREDICT: for', fsim - asim)\n",
    "elif fsim == asim:\n",
    "    print('PREDICT: undecided')\n",
    "else:\n",
    "    print('PREDICT: against', fsim - asim)\n",
    "print('ACTUAL: ', result.loc[debate_id]['winner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: after manually trying multiple debates, similairy scores worked for appromximately over 50% of the time. (E.g. d20171003, d20191112, d20180514)\n",
    "\n",
    "Todo: quantitatively computes the accuracy across all debates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Count Vectorizer Method\n",
    "Naive implementation that only checks the intersection of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Refer: adsieg's github: cosine_distance_countvectorizer_method\n",
    "def cos_cv(s1, s2, debug=False):\n",
    "    # sentences to list\n",
    "    allsentences = [s1 , s2]\n",
    "    # text to vector\n",
    "    vectorizer = CountVectorizer()\n",
    "    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n",
    "    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n",
    "    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n",
    "    # distance of similarity\n",
    "    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n",
    "    if debug:\n",
    "        print('Similarity of two sentences are equal to ',round((1-cosine)*100,2),'%')\n",
    "    return 1 - cosine\n",
    "\n",
    "# This averages the count vectorize similarities across all sentences\n",
    "# For each speech sentence, choose the max of sim between three main points\n",
    "#    - cslst: matrix of speeches's sentences\n",
    "#    - mcsents: the sentences of the main points\n",
    "def cosine_countvectorizer_avg(cslst, mcsents):\n",
    "    s = 0\n",
    "    sent_num = 0\n",
    "    for speech in cslst:\n",
    "        for sent in speech:\n",
    "            simscores = [cos_cv(sent, mpoint) for mpoint in mcsents]\n",
    "            s += max(simscores)\n",
    "        sent_num += len(speech)\n",
    "    return s / sent_num        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICT: for 0.04698983646117311\n",
      "ACTUAL:  for\n"
     ]
    }
   ],
   "source": [
    "# Most basic prediction\n",
    "fcoscv = cosine_countvectorizer_avg(fcslst, mfcsents)\n",
    "acoscv = cosine_countvectorizer_avg(acslst, macsents)\n",
    "if  fcoscv > acoscv:\n",
    "    print('PREDICT: for', fcoscv - acoscv)\n",
    "elif fcoscv == asim:\n",
    "    print('PREDICT: undecided')\n",
    "else:\n",
    "    print('PREDICT: against', fcoscv - acoscv)\n",
    "print('ACTUAL: ', result.loc[debate_id]['winner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on All Debates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>for</th>\n",
       "      <th>against</th>\n",
       "      <th>For_Main_Points</th>\n",
       "      <th>against_Main_Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d20191112</td>\n",
       "      <td>Capitalism Is a Blessing</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>['John Mackey', 'Katherine Mangu-Ward']</td>\n",
       "      <td>['Bhaskar Sunkara', 'Richard D. Wolff']</td>\n",
       "      <td>['By promoting market competition and rewardin...</td>\n",
       "      <td>['Capitalism serves the interests of large cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20191029</td>\n",
       "      <td>Parenting Is Overrated</td>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>['Robert Plomin', 'Nancy Segal']</td>\n",
       "      <td>['Paige Harden', 'Ann Pleshette Murphy']</td>\n",
       "      <td>[\"We're in the midst of a DNA revolution: Whil...</td>\n",
       "      <td>['While DNA is important, factors like familia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d20191022</td>\n",
       "      <td>Europe Has Declared War on American Tech Compa...</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>['Roslyn Layton', 'Berin Szóka']</td>\n",
       "      <td>['Marietje Schaake', 'Ramesh Srinivasan']</td>\n",
       "      <td>['European regulators have declared war on Ame...</td>\n",
       "      <td>['Brussels isn’t waging war on Silicon Valley....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d20190917</td>\n",
       "      <td>Replace Private Insurance with Medicare for All</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>['Dr. Adam Gaffney', 'Joseph Sanberg']</td>\n",
       "      <td>['Nick Gillespie', 'Sally Pipes']</td>\n",
       "      <td>['The United States government should follow t...</td>\n",
       "      <td>['Individuals should have the freedom to choos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d20190912</td>\n",
       "      <td>Unresolved: Shifting Power in the Middle East</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>['Michael Doran', 'Reuel Marc Gerecht', 'Berna...</td>\n",
       "      <td>['Brett McGurk', 'Barbara Slavin']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title        date  \\\n",
       "0  d20191112                           Capitalism Is a Blessing  2019-11-12   \n",
       "1  d20191029                             Parenting Is Overrated  2019-10-29   \n",
       "2  d20191022  Europe Has Declared War on American Tech Compa...  2019-10-22   \n",
       "3  d20190917    Replace Private Insurance with Medicare for All  2019-09-17   \n",
       "4  d20190912      Unresolved: Shifting Power in the Middle East  2019-09-12   \n",
       "\n",
       "                                                 for  \\\n",
       "0            ['John Mackey', 'Katherine Mangu-Ward']   \n",
       "1                   ['Robert Plomin', 'Nancy Segal']   \n",
       "2                   ['Roslyn Layton', 'Berin Szóka']   \n",
       "3             ['Dr. Adam Gaffney', 'Joseph Sanberg']   \n",
       "4  ['Michael Doran', 'Reuel Marc Gerecht', 'Berna...   \n",
       "\n",
       "                                     against  \\\n",
       "0    ['Bhaskar Sunkara', 'Richard D. Wolff']   \n",
       "1   ['Paige Harden', 'Ann Pleshette Murphy']   \n",
       "2  ['Marietje Schaake', 'Ramesh Srinivasan']   \n",
       "3          ['Nick Gillespie', 'Sally Pipes']   \n",
       "4         ['Brett McGurk', 'Barbara Slavin']   \n",
       "\n",
       "                                     For_Main_Points  \\\n",
       "0  ['By promoting market competition and rewardin...   \n",
       "1  [\"We're in the midst of a DNA revolution: Whil...   \n",
       "2  ['European regulators have declared war on Ame...   \n",
       "3  ['The United States government should follow t...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                 against_Main_Points  \n",
       "0  ['Capitalism serves the interests of large cor...  \n",
       "1  ['While DNA is important, factors like familia...  \n",
       "2  ['Brussels isn’t waging war on Silicon Valley....  \n",
       "3  ['Individuals should have the freedom to choos...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = meta.reset_index()\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "winners = []\n",
    "for i in range(len(meta)):\n",
    "    if pd.isnull(meta.iloc[i]['For_Main_Points']):\n",
    "        continue\n",
    "    debate_id = meta.iloc[i]['id']\n",
    "    try:\n",
    "        scripts = pd.read_csv('../For Against Scripts/for_against_scripts_' + debate_id + '.csv')\n",
    "    except:\n",
    "        print(debate_id)\n",
    "        continue\n",
    "    if len(scripts) == 0:\n",
    "        continue\n",
    "        \n",
    "    #default debate id and index, can be a different one\n",
    "#     debate_id = 'd20180514'\n",
    "    debate = meta.iloc[i]\n",
    "\n",
    "    # This prints a nicely formatted single debate info from meta\n",
    "    #print('Debate ID: ', debate_id, '\\nTopic: ', debate['title'])\n",
    "\n",
    "    # Also constructs a list of main points for both sides\n",
    "    fmains = []\n",
    "    for i, fmain in enumerate(ast.literal_eval(debate['For_Main_Points']), start=1):\n",
    "        fmains.append(fmain.lower())\n",
    "\n",
    "    amains = []\n",
    "    for i, amain in enumerate(ast.literal_eval(debate['against_Main_Points']), start=1):\n",
    "        amains.append(amain.lower())\n",
    "\n",
    "    # Fetch the debate scripts\n",
    "    scripts = pd.read_csv('../For Against Scripts/for_against_scripts_' + debate_id + '.csv')\n",
    "\n",
    "    # Construct lists of the speeches by both sides. \n",
    "    flist = [s for s in scripts.loc[scripts['side'] == 'for']['script']]\n",
    "    alist = [s for s in scripts.loc[scripts['side'] == 'against']['script']]\n",
    "\n",
    "    # The matrix of cleaned words/sentences of each speech from both sides\n",
    "    fcwlst, fcslst = clean(flist)\n",
    "    acwlst, acslst = clean(alist)\n",
    "    # The cleaned sentences and words from main points as one single speech\n",
    "    mfcwords, mfcsents = mclean(fmains)\n",
    "    macwords, macsents = mclean(amains)\n",
    "\n",
    "    # Most basic prediction\n",
    "    fcoscv = cosine_countvectorizer_avg(fcslst, mfcsents)\n",
    "    acoscv = cosine_countvectorizer_avg(acslst, macsents)\n",
    "    winner = result.loc[debate_id]['winner']\n",
    "    if winner == 'undecided':\n",
    "        continue\n",
    "    print(fcoscv, acoscv)\n",
    "    if  fcoscv > acoscv:\n",
    "        print('PREDICT: for', fcoscv - acoscv)\n",
    "        predictions.append('for')\n",
    "#     elif fcoscv == asim:\n",
    "#         continue\n",
    "#         print('PREDICT: undecided')\n",
    "    else:\n",
    "        predictions.append('against')\n",
    "        print('PREDICT: against', fcoscv - acoscv)\n",
    "    \n",
    "    winners.append(winner)\n",
    "    print('ACTUAL: ', winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5441176470588235"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.Series(predictions) == pd.Series(winners)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "against    78\n",
       "for        58\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(winners).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: after manually trying multiple debates, this method works much better than the previous naive jaccard similarity.\n",
    "\n",
    "Todo: quantitatively computes the accuracy across all debates. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
