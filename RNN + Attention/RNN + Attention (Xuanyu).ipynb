{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob \n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords?\n",
    "Combine all for points?\n",
    "What if word not exist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Word Embeddings, Attention Vectors, and Matching Vectors using GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found word vecs:  400000\n"
     ]
    }
   ],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found word vecs: ',len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.027166 , -0.1762   , -0.19623  ,  0.33527  ,  0.062392 ,\n",
       "       -0.29748  ,  0.46416  , -0.13271  ,  0.38312  , -0.37872  ,\n",
       "        0.53709  , -0.071442 , -0.15837  ,  0.049163 , -0.2225   ,\n",
       "        0.33305  ,  0.11272  , -0.10672  ,  0.48584  , -0.19743  ,\n",
       "       -0.8179   , -0.43915  , -0.68904  , -0.23316  ,  0.082357 ,\n",
       "        0.49117  , -0.08154  ,  0.10273  ,  0.39784  ,  0.68379  ,\n",
       "       -0.072532 , -0.26836  ,  0.52545  , -0.14456  , -0.3563   ,\n",
       "       -0.19699  ,  0.28968  , -0.71389  ,  0.10056  ,  0.41272  ,\n",
       "        0.52239  , -0.26027  ,  0.016722 ,  0.54869  ,  0.076753 ,\n",
       "        0.010193 , -0.11065  ,  0.47543  , -0.20591  ,  0.11673  ,\n",
       "       -0.096665 ,  0.13824  , -0.16024  , -0.43239  ,  0.23957  ,\n",
       "        0.21804  ,  0.48378  ,  0.27944  , -0.74636  , -0.60598  ,\n",
       "       -0.37709  ,  0.28384  , -0.1613   , -0.0017167,  0.35972  ,\n",
       "        0.091564 , -0.070139 , -0.9614   , -0.25841  ,  0.33928  ,\n",
       "        0.44946  , -0.1805   ,  0.051543 ,  0.26107  ,  0.16685  ,\n",
       "        0.72182  , -0.25338  , -0.685    ,  0.31886  , -0.28397  ,\n",
       "        0.1987   , -0.55063  ,  0.10243  , -0.81482  ,  0.48636  ,\n",
       "        0.19379  , -0.12213  , -0.09981  ,  0.56565  ,  0.35586  ,\n",
       "       -0.035299 ,  0.14357  ,  0.25993  , -0.33945  ,  0.47758  ,\n",
       "        0.010572 , -0.15973  , -0.37226  , -0.28782  , -0.015834 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index['unk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>for</th>\n",
       "      <th>against</th>\n",
       "      <th>For_Main_Points</th>\n",
       "      <th>against_Main_Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d20191112</td>\n",
       "      <td>Capitalism Is a Blessing</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>['John Mackey', 'Katherine Mangu-Ward']</td>\n",
       "      <td>['Bhaskar Sunkara', 'Richard D. Wolff']</td>\n",
       "      <td>['By promoting market competition and rewardin...</td>\n",
       "      <td>['Capitalism serves the interests of large cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20191029</td>\n",
       "      <td>Parenting Is Overrated</td>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>['Robert Plomin', 'Nancy Segal']</td>\n",
       "      <td>['Paige Harden', 'Ann Pleshette Murphy']</td>\n",
       "      <td>[\"We're in the midst of a DNA revolution: Whil...</td>\n",
       "      <td>['While DNA is important, factors like familia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d20191022</td>\n",
       "      <td>Europe Has Declared War on American Tech Compa...</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>['Roslyn Layton', 'Berin Szóka']</td>\n",
       "      <td>['Marietje Schaake', 'Ramesh Srinivasan']</td>\n",
       "      <td>['European regulators have declared war on Ame...</td>\n",
       "      <td>['Brussels isn’t waging war on Silicon Valley....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d20190917</td>\n",
       "      <td>Replace Private Insurance with Medicare for All</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>['Dr. Adam Gaffney', 'Joseph Sanberg']</td>\n",
       "      <td>['Nick Gillespie', 'Sally Pipes']</td>\n",
       "      <td>['The United States government should follow t...</td>\n",
       "      <td>['Individuals should have the freedom to choos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d20190912</td>\n",
       "      <td>Unresolved: Shifting Power in the Middle East</td>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>['Michael Doran', 'Reuel Marc Gerecht', 'Berna...</td>\n",
       "      <td>['Brett McGurk', 'Barbara Slavin']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title        date  \\\n",
       "0  d20191112                           Capitalism Is a Blessing  2019-11-12   \n",
       "1  d20191029                             Parenting Is Overrated  2019-10-29   \n",
       "2  d20191022  Europe Has Declared War on American Tech Compa...  2019-10-22   \n",
       "3  d20190917    Replace Private Insurance with Medicare for All  2019-09-17   \n",
       "4  d20190912      Unresolved: Shifting Power in the Middle East  2019-09-12   \n",
       "\n",
       "                                                 for  \\\n",
       "0            ['John Mackey', 'Katherine Mangu-Ward']   \n",
       "1                   ['Robert Plomin', 'Nancy Segal']   \n",
       "2                   ['Roslyn Layton', 'Berin Szóka']   \n",
       "3             ['Dr. Adam Gaffney', 'Joseph Sanberg']   \n",
       "4  ['Michael Doran', 'Reuel Marc Gerecht', 'Berna...   \n",
       "\n",
       "                                     against  \\\n",
       "0    ['Bhaskar Sunkara', 'Richard D. Wolff']   \n",
       "1   ['Paige Harden', 'Ann Pleshette Murphy']   \n",
       "2  ['Marietje Schaake', 'Ramesh Srinivasan']   \n",
       "3          ['Nick Gillespie', 'Sally Pipes']   \n",
       "4         ['Brett McGurk', 'Barbara Slavin']   \n",
       "\n",
       "                                     For_Main_Points  \\\n",
       "0  ['By promoting market competition and rewardin...   \n",
       "1  [\"We're in the midst of a DNA revolution: Whil...   \n",
       "2  ['European regulators have declared war on Ame...   \n",
       "3  ['The United States government should follow t...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                 against_Main_Points  \n",
       "0  ['Capitalism serves the interests of large cor...  \n",
       "1  ['While DNA is important, factors like familia...  \n",
       "2  ['Brussels isn’t waging war on Silicon Valley....  \n",
       "3  ['Individuals should have the freedom to choos...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_points =pd.read_csv('DebateStar/Meta Data/metadata_appended_main_points.csv') \n",
    "main_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_points(fid, main_points):\n",
    "    \"\"\"\n",
    "    Returns main points for both sides given fid and main_points table\n",
    "    \"\"\"\n",
    "    for_points = main_points.loc[main_points['id'] == fid]['For_Main_Points'].item()\n",
    "    for_points = ast.literal_eval(for_points)\n",
    "    against_points = main_points.loc[main_points['id'] == fid]['against_Main_Points'].item()\n",
    "    against_points = ast.literal_eval(against_points)\n",
    "    return for_points, against_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "def tokenize_point(point):\n",
    "    \"\"\"\n",
    "    Returns a list of lowercased tokens, without punctuations\n",
    "    \"\"\"\n",
    "    tokens = [t for t in word_tokenize(point.lower()) if t not in punctuations]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2glove(word, embedding_index):\n",
    "    \"\"\"\n",
    "    Returns GloVe embedding given a word, and zeros if word does not exist in the dictionary\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    try:\n",
    "        word_vec = embedding_index[word]\n",
    "    except:\n",
    "        word_vec = embedding_index['unk']\n",
    "    return word_vec\n",
    "\n",
    "def point2gloves(point, embedding_index):\n",
    "    \"\"\"\n",
    "    Returns a list of word embeddings given a sentence\n",
    "    \"\"\"\n",
    "    word_vecs = np.array([word2glove(word, embedding_index) for word in tokenize_point(point)])\n",
    "    return word_vecs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attention_vector(word, embedding_index, title_vecs):\n",
    "    \"\"\"\n",
    "    Returns the attention vector of a word w.r.t. the title (a_k in paper)\n",
    "    \"\"\"\n",
    "    word_vec = word2glove(word, embedding_index)\n",
    "    #weights = np.dot([word_vec], title_vecs)\n",
    "    # normalized dot product, then softmax as weights\n",
    "    weights = np.sum(word_vec * title_vecs, axis=1) / np.linalg.norm(title_vecs, axis=1)\n",
    "#     print(weights)\n",
    "#     print(np.exp(weights) / np.sum(np.exp(weights)))\n",
    "    weights = np.exp(weights) / np.sum(np.exp(weights))\n",
    "    attention_vec = np.matmul(title_vecs.T, weights.reshape((-1,1))).T[0]\n",
    "    return attention_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.concatenate((attention_vector('king', embedding_index, title_vecs), \\\n",
    "#                word2glove('the', embedding_index)), axis = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_vector(attention_vec, word, embedding_index):\n",
    "    \"\"\"\n",
    "    Returns the matching vector of a word w.r.t. the title by concatenating the \n",
    "    resulting attention vector and word embedding(m_k in paper)\n",
    "    \"\"\"\n",
    "    matching_vec = np.concatenate((attention_vec, word2glove(word, embedding_index)), axis = None)\n",
    "    return matching_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate Title: Capitalism Is a Blessing\n"
     ]
    }
   ],
   "source": [
    "fid = 'd20191112'\n",
    "title = main_points.loc[main_points['id'] == fid]['title'].iloc[0]\n",
    "for_points, against_points = get_main_points(fid, main_points)\n",
    "print('Debate Title:', title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'socialism'\n",
    "title_vecs = point2gloves(title, embedding_index)\n",
    "attention_vec = attention_vector(word, embedding_index, title_vecs)\n",
    "matching_vec = matching_vector(attention_vec, word, embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: socialism\n",
      "Similarities between attention vector and words in title: [[0.99657255 0.3246895  0.23374872 0.22429445]]\n"
     ]
    }
   ],
   "source": [
    "print('word:', word)\n",
    "print('Similarities between attention vector and words in title:', cosine_similarity([attention_vec], title_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point2mn(point, title, embedding_index):\n",
    "    #point_vecs = point2gloves(point, embedding_index)\n",
    "    words = tokenize_point(point)\n",
    "    title_vecs = point2gloves(title, embedding_index)\n",
    "    nn_inputs = []\n",
    "    for word in words:\n",
    "        attention_vec = attention_vector(word, embedding_index, title_vecs)\n",
    "        matching_vec = matching_vector(attention_vec, word, embedding_index)\n",
    "        nn_inputs.append(matching_vec)\n",
    "    return np.array(nn_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = word2mn(point, title, embedding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
