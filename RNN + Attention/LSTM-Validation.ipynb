{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + Fully Connected Layer\n",
    "\n",
    "This file streamlines the LSTM and fully connected layer process. It validates the for/against vectors based on results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fine-tune embedding\n",
    "\n",
    "2. Same initial hidden states (DONE)\n",
    "\n",
    "3. Check dropout in evaluation (DONE, no dropout) \n",
    "\n",
    "4. Validation set (DONE)\n",
    "\n",
    "4. Check fully connected weights \n",
    "\n",
    "6. Add f.c. layers (DOING)\n",
    "\n",
    "7. Mean + std of several runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11b568f50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from point2mn import point2mn\n",
    "from point2mn import get_main_points\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the glove embedding, fine-tune to be done.\n",
    "def generate_embedding(filename='glove.6B.100d.txt'):\n",
    "    embedding_index = {}\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:],dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Formed word vecs from', filename, \": \", len(embedding_index))\n",
    "    return embedding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formed word vecs from glove.6B.100d.txt :  400000\n"
     ]
    }
   ],
   "source": [
    "embedding_index = generate_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM methods\n",
    "def run_lstm_one_sentence(sentence, title, lstm, hidden, embedding_index):\n",
    "    inputs = point2mn(sentence, title, embedding_index)\n",
    "    tensor_input = [torch.tensor([x]) for x in inputs]\n",
    "    tensor_input = torch.cat(tensor_input).view(1, len(tensor_input), -1)\n",
    "    #hidden = (torch.randn(1, 1, HIDDEN_DIM), torch.randn(1, 1, HIDDEN_DIM)) #???????\n",
    "    tensor_output, hidden = lstm(tensor_input, hidden)\n",
    "    # should return hidden state and cell state in 'hidden' instead of output\n",
    "    return hidden\n",
    "\n",
    "def run_lstm_on_fid(fid, embedding_index, lstm, hidden, combine_func = [torch.mean, torch.max]):\n",
    "    \"\"\"\n",
    "    Given fid, find main points for the debate and for each sentence, pass corresponding \n",
    "    matching vectors to lstm and get hidden state in the end. Gather the hidden states\n",
    "    in a list and do combine_func.\n",
    "    -combine_func: the funtion applied to combine lstm outputs from one side elementwisely\n",
    "    \"\"\"\n",
    "    \n",
    "    title = main_points[main_points['id'] == fid].title.iloc[0]\n",
    "    for_main_points, against_main_points = get_main_points(fid, main_points)\n",
    "    for_output_list = []\n",
    "    against_output_list = []\n",
    "    \n",
    "    for sentence in for_main_points:\n",
    "        hidden_state, cell_state = run_lstm_one_sentence(sentence, title, lstm, hidden, embedding_index)\n",
    "        for_output_list.append(hidden_state)\n",
    "        \n",
    "    for sentence in against_main_points:\n",
    "        hidden_state, cell_state = run_lstm_one_sentence(sentence, title, lstm, hidden, embedding_index)\n",
    "        against_output_list.append(hidden_state)\n",
    "    for_torchs = []\n",
    "    against_torchs = []\n",
    "    for combine_f in combine_func:\n",
    "        if combine_f == torch.mean:\n",
    "            for_torch = combine_f(torch.stack(for_output_list), dim = 0, keepdim = True)#[0]\n",
    "            against_torch = combine_f(torch.stack(against_output_list), dim = 0, keepdim = True)#[0]\n",
    "        else:\n",
    "            for_torch = combine_f(torch.stack(for_output_list), dim = 0, keepdim = True)[0]\n",
    "            against_torch = combine_f(torch.stack(against_output_list), dim = 0, keepdim = True)[0]\n",
    "        for_torchs.append(for_torch)\n",
    "        against_torchs.append(against_torch)\n",
    "    if len(for_torchs) == 2:\n",
    "        t1 = torch.cat((for_torchs[0], for_torchs[1]), dim = 2)\n",
    "        t2 = torch.cat((against_torchs[0], against_torchs[1]), dim = 2)\n",
    "        return torch.cat((t1, t2), dim = 2)\n",
    "    return torch.cat((for_torchs[0], against_torchs[0]), dim = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, output_size_1, output_size_2, n_layers, embedding_dim, hidden_dim, combine_funcs, hidden, drop_prob=0.5):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.output_size_1 = output_size_1\n",
    "        self.output_size_2 = output_size_2\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.combine_funcs = combine_funcs\n",
    "        self.hidden = hidden\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 * len(self.combine_funcs), self.output_size_1)\n",
    "        #self.fc2 = nn.Linear(output_size_1, self.output_size_2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, fids):\n",
    "        batch_size = len(fids)\n",
    "        lstm_out = torch.stack([run_lstm_on_fid(fid, embedding_index, self.lstm, self.hidden, \\\n",
    "                                                combine_func = self.combine_funcs) for fid in fids])\n",
    "    \n",
    "        lstm_out = lstm_out.contiguous().view(-1, len(self.combine_funcs) * 2 * self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_results(outputsize_1, outputsize_2, N_LAYERS, embedding_dim, hidden_dim, combine_funcs, drop_prob, lr, epochs):\n",
    "    print('output size 1 =', outputsize_1, 'output size 2 =', outputsize_2,'n_layers =', N_LAYERS, 'embedding_dim = ', embedding_dim, \\\n",
    "         'hidden_dim = ', hidden_dim, 'combine_funcs = ', str(combine_funcs), 'drop_prob = ', drop_prob, \\\n",
    "          'lr = ', lr, 'epochs = ', epochs)\n",
    "    return pd.DataFrame({'# Epoch': range(1,epochs + 1), 'training acc': train_acc,\\\n",
    "                         'validation acc': val_acc, 'testing acc': test_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Data of debates\n",
    "main_points = pd.read_csv('../Meta Data/metadata_appended_main_points.csv') \n",
    "main_points.dropna(subset = ['For_Main_Points', 'against_Main_Points'], inplace = True)\n",
    "main_points = main_points.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>for</th>\n",
       "      <th>against</th>\n",
       "      <th>For_Main_Points</th>\n",
       "      <th>against_Main_Points</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d20191112</td>\n",
       "      <td>Capitalism Is a Blessing</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>['John Mackey', 'Katherine Mangu-Ward']</td>\n",
       "      <td>['Bhaskar Sunkara', 'Richard D. Wolff']</td>\n",
       "      <td>['By promoting market competition and rewardin...</td>\n",
       "      <td>['Capitalism serves the interests of large cor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20191029</td>\n",
       "      <td>Parenting Is Overrated</td>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>['Robert Plomin', 'Nancy Segal']</td>\n",
       "      <td>['Paige Harden', 'Ann Pleshette Murphy']</td>\n",
       "      <td>[\"We're in the midst of a DNA revolution: Whil...</td>\n",
       "      <td>['While DNA is important, factors like familia...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                     title        date  \\\n",
       "0  d20191112  Capitalism Is a Blessing  2019-11-12   \n",
       "1  d20191029    Parenting Is Overrated  2019-10-29   \n",
       "\n",
       "                                       for  \\\n",
       "0  ['John Mackey', 'Katherine Mangu-Ward']   \n",
       "1         ['Robert Plomin', 'Nancy Segal']   \n",
       "\n",
       "                                    against  \\\n",
       "0   ['Bhaskar Sunkara', 'Richard D. Wolff']   \n",
       "1  ['Paige Harden', 'Ann Pleshette Murphy']   \n",
       "\n",
       "                                     For_Main_Points  \\\n",
       "0  ['By promoting market competition and rewardin...   \n",
       "1  [\"We're in the midst of a DNA revolution: Whil...   \n",
       "\n",
       "                                 against_Main_Points  label  \n",
       "0  ['Capitalism serves the interests of large cor...    0.0  \n",
       "1  ['While DNA is important, factors like familia...    1.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the winner label\n",
    "results = pd.read_csv('../results_data/final_online.csv') \n",
    "results = results.loc[results['winner'].apply(lambda x: x != 'undecided')]\n",
    "results['winner'] = results['winner'].apply(lambda x: 1 if x == 'for' else 0)\n",
    "id2winner = results[['id', 'winner']].set_index('id').to_dict()['winner']\n",
    "\n",
    "main_points['label'] = main_points.id.apply(lambda x: id2winner[x] if x in id2winner else np.nan)\n",
    "main_points = main_points.dropna(subset = ['label']).reset_index(drop = True)\n",
    "main_points.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train: 85 \n",
      "X Validation:  10 \n",
      "X Test:  24\n"
     ]
    }
   ],
   "source": [
    "# Input and Train/Vadidation/Test\n",
    "inputs = main_points.id.tolist()\n",
    "labels =  main_points.label.tolist()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size = 0.2, random_state = 5)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 0)\n",
    "\n",
    "y_train = torch.tensor(y_train)\n",
    "y_val = torch.tensor(y_val)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "print(\"X Train:\", len(X_train), \"\\nX Validation: \", len(X_val), \"\\nX Test: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measureament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 50\n",
    "# OUTPUT_SIZE_1 = 20\n",
    "OUTPUT_SIZE_1 = 1\n",
    "COMBINE_FUNCS =[torch.mean, torch.max]\n",
    "OUTPUT_SIZE_2 = 1\n",
    "N_LAYERS = 1\n",
    "DROPOUT_PROB = 0.5\n",
    "HIDDEN_INITIAL = (torch.randn(1, 1, HIDDEN_DIM), torch.randn(1, 1, HIDDEN_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet(OUTPUT_SIZE_1, OUTPUT_SIZE_2, N_LAYERS, EMBEDDING_DIM, HIDDEN_DIM, COMBINE_FUNCS, HIDDEN_INITIAL,DROPOUT_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "counter = 0\n",
    "train_acc = []\n",
    "train_acc2 = []\n",
    "\n",
    "val_acc = []\n",
    "test_acc = []\n",
    "model.train()\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    output = model(X_train)\n",
    "    loss = criterion(output.squeeze(), y_train.float())\n",
    "    #print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    outs = model(X_train)\n",
    "    train_acc.append(torch.sum((outs > 0.5) == y_train).item() / len(y_train))\n",
    "\n",
    "    outs = model(X_val)\n",
    "    val_acc.append(torch.sum((outs > 0.5) == y_val).item() / len(y_val))\n",
    "    \n",
    "    outs = model(X_test)\n",
    "    test_acc.append(torch.sum((outs > 0.5) == y_test).item() / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size 1 = 20 output size 2 = 1 n_layers = 1 embedding_dim =  200 hidden_dim =  50 combine_funcs =  [<built-in method mean of type object at 0x11ae27eb0>, <built-in method max of type object at 0x11ae27eb0>] drop_prob =  0.5 lr =  0.005 epochs =  20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Epoch</th>\n",
       "      <th>training acc</th>\n",
       "      <th>validation acc</th>\n",
       "      <th>testing acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    # Epoch  training acc  validation acc  testing acc\n",
       "0         1      0.658824             0.6     0.583333\n",
       "1         2      0.800000             0.6     0.708333\n",
       "2         3      0.823529             0.6     0.666667\n",
       "3         4      0.858824             0.6     0.666667\n",
       "4         5      0.752941             0.7     0.500000\n",
       "5         6      0.858824             0.6     0.625000\n",
       "6         7      0.717647             0.7     0.583333\n",
       "7         8      0.870588             0.6     0.541667\n",
       "8         9      0.882353             0.6     0.541667\n",
       "9        10      0.905882             0.7     0.583333\n",
       "10       11      0.835294             0.8     0.541667\n",
       "11       12      0.894118             0.7     0.625000\n",
       "12       13      0.929412             0.7     0.666667\n",
       "13       14      0.858824             0.6     0.500000\n",
       "14       15      1.000000             0.7     0.750000\n",
       "15       16      0.988235             0.6     0.625000\n",
       "16       17      1.000000             0.7     0.625000\n",
       "17       18      1.000000             0.7     0.541667\n",
       "18       19      1.000000             0.7     0.666667\n",
       "19       20      0.988235             0.7     0.583333"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_results(OUTPUT_SIZE_1, OUTPUT_SIZE_2, N_LAYERS, EMBEDDING_DIM, HIDDEN_DIM, [torch.mean, torch.max], DROPOUT_PROB, lr, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the fully connected layer weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight_acc(weight_t, DIM=200):\n",
    "    D = int(DIM / 2)\n",
    "    positive = len([1 for i in weight_t[0][:D] if i > 0])\n",
    "    negative = len([1 for i in weight_t[0][D:] if i < 0])\n",
    "    print(\"TP: \", positive / 100, \"TN: \", negative / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n",
      "TP:  0.5 TN:  0.53\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "counter = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # get weight\n",
    "    compute_weight_acc(model.fc1.weight)\n",
    "    #print(model.fc1.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
