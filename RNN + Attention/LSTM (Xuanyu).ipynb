{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ef1ab70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from point2mn import point2mn\n",
    "from point2mn import get_main_points\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found word vecs:  400000\n"
     ]
    }
   ],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "f = open('../../glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found word vecs: ',len(embedding_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>for</th>\n",
       "      <th>against</th>\n",
       "      <th>For_Main_Points</th>\n",
       "      <th>against_Main_Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d20191112</td>\n",
       "      <td>Capitalism Is a Blessing</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>['John Mackey', 'Katherine Mangu-Ward']</td>\n",
       "      <td>['Bhaskar Sunkara', 'Richard D. Wolff']</td>\n",
       "      <td>['By promoting market competition and rewardin...</td>\n",
       "      <td>['Capitalism serves the interests of large cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20191029</td>\n",
       "      <td>Parenting Is Overrated</td>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>['Robert Plomin', 'Nancy Segal']</td>\n",
       "      <td>['Paige Harden', 'Ann Pleshette Murphy']</td>\n",
       "      <td>[\"We're in the midst of a DNA revolution: Whil...</td>\n",
       "      <td>['While DNA is important, factors like familia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d20191022</td>\n",
       "      <td>Europe Has Declared War on American Tech Compa...</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>['Roslyn Layton', 'Berin Szóka']</td>\n",
       "      <td>['Marietje Schaake', 'Ramesh Srinivasan']</td>\n",
       "      <td>['European regulators have declared war on Ame...</td>\n",
       "      <td>['Brussels isn’t waging war on Silicon Valley....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d20190917</td>\n",
       "      <td>Replace Private Insurance with Medicare for All</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>['Dr. Adam Gaffney', 'Joseph Sanberg']</td>\n",
       "      <td>['Nick Gillespie', 'Sally Pipes']</td>\n",
       "      <td>['The United States government should follow t...</td>\n",
       "      <td>['Individuals should have the freedom to choos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d20190802</td>\n",
       "      <td>The Recent U.S. Policy Towards China Is Produc...</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>['Michael Pillsbury', 'Kori Schake']</td>\n",
       "      <td>['Graham Allison', 'Jake Sullivan']</td>\n",
       "      <td>['The prospect of China becoming an open and l...</td>\n",
       "      <td>['The United States and China are great compet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title        date  \\\n",
       "0  d20191112                           Capitalism Is a Blessing  2019-11-12   \n",
       "1  d20191029                             Parenting Is Overrated  2019-10-29   \n",
       "2  d20191022  Europe Has Declared War on American Tech Compa...  2019-10-22   \n",
       "3  d20190917    Replace Private Insurance with Medicare for All  2019-09-17   \n",
       "4  d20190802  The Recent U.S. Policy Towards China Is Produc...  2019-08-02   \n",
       "\n",
       "                                       for  \\\n",
       "0  ['John Mackey', 'Katherine Mangu-Ward']   \n",
       "1         ['Robert Plomin', 'Nancy Segal']   \n",
       "2         ['Roslyn Layton', 'Berin Szóka']   \n",
       "3   ['Dr. Adam Gaffney', 'Joseph Sanberg']   \n",
       "4     ['Michael Pillsbury', 'Kori Schake']   \n",
       "\n",
       "                                     against  \\\n",
       "0    ['Bhaskar Sunkara', 'Richard D. Wolff']   \n",
       "1   ['Paige Harden', 'Ann Pleshette Murphy']   \n",
       "2  ['Marietje Schaake', 'Ramesh Srinivasan']   \n",
       "3          ['Nick Gillespie', 'Sally Pipes']   \n",
       "4        ['Graham Allison', 'Jake Sullivan']   \n",
       "\n",
       "                                     For_Main_Points  \\\n",
       "0  ['By promoting market competition and rewardin...   \n",
       "1  [\"We're in the midst of a DNA revolution: Whil...   \n",
       "2  ['European regulators have declared war on Ame...   \n",
       "3  ['The United States government should follow t...   \n",
       "4  ['The prospect of China becoming an open and l...   \n",
       "\n",
       "                                 against_Main_Points  \n",
       "0  ['Capitalism serves the interests of large cor...  \n",
       "1  ['While DNA is important, factors like familia...  \n",
       "2  ['Brussels isn’t waging war on Silicon Valley....  \n",
       "3  ['Individuals should have the freedom to choos...  \n",
       "4  ['The United States and China are great compet...  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_points = pd.read_csv('../Meta Data/metadata_appended_main_points.csv') \n",
    "main_points.dropna(subset = ['For_Main_Points', 'against_Main_Points'], inplace = True)\n",
    "main_points = main_points.reset_index(drop = True)\n",
    "main_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('../results_data/final_online.csv') \n",
    "results = results.loc[results['winner'].apply(lambda x: x != 'undecided')]\n",
    "results['winner'] = results['winner'].apply(lambda x: 1 if x == 'for' else 0)\n",
    "id2winner = results[['id', 'winner']].set_index('id').to_dict()['winner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>for</th>\n",
       "      <th>against</th>\n",
       "      <th>For_Main_Points</th>\n",
       "      <th>against_Main_Points</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d20191112</td>\n",
       "      <td>Capitalism Is a Blessing</td>\n",
       "      <td>2019-11-12</td>\n",
       "      <td>['John Mackey', 'Katherine Mangu-Ward']</td>\n",
       "      <td>['Bhaskar Sunkara', 'Richard D. Wolff']</td>\n",
       "      <td>['By promoting market competition and rewardin...</td>\n",
       "      <td>['Capitalism serves the interests of large cor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d20191029</td>\n",
       "      <td>Parenting Is Overrated</td>\n",
       "      <td>2019-10-29</td>\n",
       "      <td>['Robert Plomin', 'Nancy Segal']</td>\n",
       "      <td>['Paige Harden', 'Ann Pleshette Murphy']</td>\n",
       "      <td>[\"We're in the midst of a DNA revolution: Whil...</td>\n",
       "      <td>['While DNA is important, factors like familia...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d20191022</td>\n",
       "      <td>Europe Has Declared War on American Tech Compa...</td>\n",
       "      <td>2019-10-22</td>\n",
       "      <td>['Roslyn Layton', 'Berin Szóka']</td>\n",
       "      <td>['Marietje Schaake', 'Ramesh Srinivasan']</td>\n",
       "      <td>['European regulators have declared war on Ame...</td>\n",
       "      <td>['Brussels isn’t waging war on Silicon Valley....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d20190917</td>\n",
       "      <td>Replace Private Insurance with Medicare for All</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>['Dr. Adam Gaffney', 'Joseph Sanberg']</td>\n",
       "      <td>['Nick Gillespie', 'Sally Pipes']</td>\n",
       "      <td>['The United States government should follow t...</td>\n",
       "      <td>['Individuals should have the freedom to choos...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d20190802</td>\n",
       "      <td>The Recent U.S. Policy Towards China Is Produc...</td>\n",
       "      <td>2019-08-02</td>\n",
       "      <td>['Michael Pillsbury', 'Kori Schake']</td>\n",
       "      <td>['Graham Allison', 'Jake Sullivan']</td>\n",
       "      <td>['The prospect of China becoming an open and l...</td>\n",
       "      <td>['The United States and China are great compet...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title        date  \\\n",
       "0  d20191112                           Capitalism Is a Blessing  2019-11-12   \n",
       "1  d20191029                             Parenting Is Overrated  2019-10-29   \n",
       "2  d20191022  Europe Has Declared War on American Tech Compa...  2019-10-22   \n",
       "3  d20190917    Replace Private Insurance with Medicare for All  2019-09-17   \n",
       "4  d20190802  The Recent U.S. Policy Towards China Is Produc...  2019-08-02   \n",
       "\n",
       "                                       for  \\\n",
       "0  ['John Mackey', 'Katherine Mangu-Ward']   \n",
       "1         ['Robert Plomin', 'Nancy Segal']   \n",
       "2         ['Roslyn Layton', 'Berin Szóka']   \n",
       "3   ['Dr. Adam Gaffney', 'Joseph Sanberg']   \n",
       "4     ['Michael Pillsbury', 'Kori Schake']   \n",
       "\n",
       "                                     against  \\\n",
       "0    ['Bhaskar Sunkara', 'Richard D. Wolff']   \n",
       "1   ['Paige Harden', 'Ann Pleshette Murphy']   \n",
       "2  ['Marietje Schaake', 'Ramesh Srinivasan']   \n",
       "3          ['Nick Gillespie', 'Sally Pipes']   \n",
       "4        ['Graham Allison', 'Jake Sullivan']   \n",
       "\n",
       "                                     For_Main_Points  \\\n",
       "0  ['By promoting market competition and rewardin...   \n",
       "1  [\"We're in the midst of a DNA revolution: Whil...   \n",
       "2  ['European regulators have declared war on Ame...   \n",
       "3  ['The United States government should follow t...   \n",
       "4  ['The prospect of China becoming an open and l...   \n",
       "\n",
       "                                 against_Main_Points  label  \n",
       "0  ['Capitalism serves the interests of large cor...    0.0  \n",
       "1  ['While DNA is important, factors like familia...    1.0  \n",
       "2  ['Brussels isn’t waging war on Silicon Valley....    0.0  \n",
       "3  ['Individuals should have the freedom to choos...    0.0  \n",
       "4  ['The United States and China are great compet...    0.0  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_points['label'] = main_points.id.apply(lambda x: id2winner[x] if x in id2winner else np.nan)\n",
    "main_points = main_points.dropna(subset = ['label']).reset_index(drop = True)\n",
    "main_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = main_points.id.tolist()\n",
    "labels =  main_points.label.tolist()\n",
    "# labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 200\n",
    "OUTPUT_SIZE = 1\n",
    "N_LAYERS = 1\n",
    "# all should use the same one lstm layer\n",
    "lstm = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden state initialize differently every time running LSTM??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_one_sentence(sentence, title, lstm, embedding_index):\n",
    "    inputs = point2mn(sentence, title, embedding_index)\n",
    "    tensor_input = [torch.tensor([x]) for x in inputs]\n",
    "    tensor_input = torch.cat(tensor_input).view(1, len(tensor_input), -1)\n",
    "#     print(tensor_input.shape)\n",
    "    hidden = (torch.randn(1, 1, HIDDEN_DIM), torch.randn(1, 1, HIDDEN_DIM)) #???????\n",
    "    tensor_output, hidden = lstm(tensor_input, hidden)\n",
    "    # should return hidden state and cell state in 'hidden' instead of output\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_lstm_on_fid(fid, embedding_index, lstm, combine_func = torch.mean):\n",
    "    \"\"\"\n",
    "    Given fid, find main points for the debate and for each sentence, pass corresponding \n",
    "    matching vectors to lstm and get hidden state in the end. Gather the hidden states\n",
    "    in a list and do combine_func.\n",
    "    -combine_func: the funtion applied to combine lstm outputs from one side elementwisely\n",
    "    \"\"\"\n",
    "    \n",
    "    title = main_points[main_points['id'] == fid].title.iloc[0]\n",
    "    for_main_points, against_main_points = get_main_points(fid, main_points)\n",
    "    for_output_list = []\n",
    "    against_output_list = []\n",
    "    \n",
    "    for sentence in for_main_points:\n",
    "        hidden_state, cell_state = run_lstm_one_sentence(sentence, title, lstm, embedding_index)\n",
    "        for_output_list.append(hidden_state)\n",
    "        \n",
    "    for sentence in against_main_points:\n",
    "        hidden_state, cell_state = run_lstm_one_sentence(sentence, title, lstm, embedding_index)\n",
    "        against_output_list.append(hidden_state)\n",
    "        \n",
    "    if combine_func == torch.mean:\n",
    "        for_torch = combine_func(torch.stack(for_output_list), dim = 0, keepdim = True)#[0]\n",
    "        against_torch = combine_func(torch.stack(against_output_list), dim = 0, keepdim = True)#[0]\n",
    "    else:\n",
    "        for_torch = combine_func(torch.stack(for_output_list), dim = 0, keepdim = True)[0]\n",
    "        against_torch = combine_func(torch.stack(against_output_list), dim = 0, keepdim = True)[0]\n",
    "    return torch.cat((for_torch, against_torch), dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fid = 'd20191112'\n",
    "# title = main_points[main_points['id'] == fid].title[0]\n",
    "# fid_torch = run_lstm_on_fid(fid, embedding_index, lstm,  combine_func = torch.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out = [run_lstm_on_fid(fid, embedding_index, lstm, combine_func = torch.mean) for fid in main_points.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([154, 400])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(lstm_out).contiguous().view(-1, HIDDEN_DIM).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, output_size, n_layers, embedding_dim, hidden_dim, drop_prob=0.5):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
    "#         self.softmax = nn.Softmax(dim = 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x, hidden):\n",
    "#     def forward(self, fids, hidden):\n",
    "    def forward(self, fids):\n",
    "        batch_size = len(fids)\n",
    "#         x = x.long()\n",
    "        lstm_out = torch.stack([run_lstm_on_fid(fid, embedding_index, self.lstm, combine_func = torch.max) \\\n",
    "                                for fid in fids])\n",
    "    \n",
    "#         lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, 2 * self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "#         out = self.softmax(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out#, hidden\n",
    "    \n",
    "#     def init_hidden(self, batch_size):\n",
    "#         weight = next(self.parameters()).data\n",
    "#         hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "#                       weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "#         return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNet(OUTPUT_SIZE, N_LAYERS, EMBEDDING_DIM, HIDDEN_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6975, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6934, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6923, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6907, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6822, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6742, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6755, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6493, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6919, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6338, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "#     h = model.init_hidden(batch_size)\n",
    "    \n",
    "#     for inputs, labels in train_loader:\n",
    "#         counter += 1\n",
    "#         h = tuple([e.data for e in h])\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "    model.zero_grad()\n",
    "    output = model(X_train)\n",
    "    loss = criterion(output.squeeze(), y_train.float())\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    outs = model(X_train)\n",
    "    train_acc.append(torch.sum((outs > 0.5) == y_train).item() / len(y_train))\n",
    "    outs = model(X_test)\n",
    "    test_acc.append(torch.sum((outs > 0.5) == y_test).item() / len(y_test))\n",
    "    \n",
    "    \n",
    "#         if counter%print_every == 0:\n",
    "#             val_h = model.init_hidden(batch_size)\n",
    "#             val_losses = []\n",
    "#             model.eval()\n",
    "#             for inp, lab in val_loader:\n",
    "#                 val_h = tuple([each.data for each in val_h])\n",
    "#                 inp, lab = inp.to(device), lab.to(device)\n",
    "#                 out, val_h = model(inp, val_h)\n",
    "#                 val_loss = criterion(out.squeeze(), lab.float())\n",
    "#                 val_losses.append(val_loss.item())\n",
    "                \n",
    "#             model.train()\n",
    "#             print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "#                   \"Step: {}...\".format(counter),\n",
    "#                   \"Loss: {:.6f}...\".format(loss.item()),\n",
    "#                   \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "#             if np.mean(val_losses) <= valid_loss_min:\n",
    "#                 torch.save(model.state_dict(), './state_dict.pt')\n",
    "#                 print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "#                 valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4842105263157895"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outs = model(X_train)\n",
    "# torch.sum((outs > 0.5) == y_train).item() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5416666666666666"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.541667\n",
       "0.0    0.458333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5020, 0.3077, 0.3503, 0.5805, 0.4688, 0.5605, 0.1735, 0.3698, 0.3980,\n",
       "        0.5669, 0.5357, 0.4020, 0.3885, 0.4056, 0.3149, 0.6157, 0.5548, 0.5643,\n",
       "        0.3511, 0.5886, 0.3619, 0.5594, 0.4987, 0.6190],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FullyNet(nn.Module):\n",
    "#     def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "#         super(FullyNet, self).__init__()\n",
    "#         self.output_size = output_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.hidden_dim = hidden_dim\n",
    "        \n",
    "#         #self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "#         self.dropout = nn.Dropout(drop_prob)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_size)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "#     def forward(self, x, hidden):\n",
    "#         batch_size = x.size(0)\n",
    "#         x = x.long()\n",
    "#         #embeds = self.embedding(x)\n",
    "#         lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "#         lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "#         out = self.dropout(lstm_out)\n",
    "#         out = self.fc(out)\n",
    "#         out = self.sigmoid(out)\n",
    "        \n",
    "#         out = out.view(batch_size, -1)\n",
    "#         out = out[:,-1]\n",
    "#         return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
